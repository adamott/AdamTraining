{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtXbYCVf8YV0MQuYJXuR7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamott/AdamTraining/blob/main/Day8_AI_Models__Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YqObXDhqUx1",
        "outputId": "61aa128c-d667-48a5-d066-563ba4672abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Adam and I wouldlike to know more about a possible cure for MS. I have had it for 8 years and I have been on numerous drugs. I am wondering if there are any medications that would be effective in controlling the disease. I have researched this subject for more than a year, and I have tried many of the medications that the doctors recommend. I have tried so many of these medications that I would probably go crazy, if I didn't already feel that I was so confused\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a text generation pipeline using a small, fast, open-access model\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\")\n",
        "# Example prompt\n",
        "prompt = \"My name is Adam and I wouldlike to know more about\"\n",
        "\n",
        "# Generate text\n",
        "output = generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Display result\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "source": [
        "!pip install -q transformers gradio\n",
        "\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Load a free summarization model from Hugging Face\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "def summarize_text(text):\n",
        "    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Build the web app\n",
        "demo = gr.Interface(\n",
        "    fn=summarize_text,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Free AI Text Summarizer\",\n",
        "    description=\"Paste any article or paragraph and get a summary using Hugging Face.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "glYFYRdxyLBs",
        "outputId": "deb7c37b-17f3-4d6c-83fb-eb83f971ccc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e53d30f08ec705da4d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e53d30f08ec705da4d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "source": [
        "model_options = {\n",
        "    \"DistilBART (fast, general-purpose)\": \"sshleifer/distilbart-cnn-12-6\",\n",
        "    \"BART Large (more accurate, slower)\": \"facebook/bart-large-cnn\"\n",
        "}\n",
        "\n",
        "def summarize_with_model(text, model_name):\n",
        "    chosen_model = pipeline(\"summarization\", model=model_options[model_name])\n",
        "    summary = chosen_model(text, max_length=130, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "gr.Interface(\n",
        "    fn=summarize_with_model,\n",
        "    inputs=[gr.Textbox(label=\"Enter text to summarize\"), gr.Dropdown(label=\"Choose Model\", choices=list(model_options.keys()))],\n",
        "    outputs=\"text\",\n",
        "    title=\"Multi-Model Text Summarizer\",\n",
        "    description=\"Try two different summarization models for comparison.\",\n",
        ").launch(share=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "7QH36G9m5s9Q",
        "outputId": "32d617a1-d6b6-45b3-a2e0-d92917ccc58f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://72911ed0ab9b505335.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://72911ed0ab9b505335.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9OTCRgAK5b7V"
      }
    }
  ]
}